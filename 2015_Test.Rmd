---
title: "2015 Test"
author: "Lake Vitton"
date: "June 16th, 2018"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gmodels)
library(stringi)
library(plyr)
library(caret)
library(C50)
library(class)
library(kernlab)
library(randomForest)
library(irr)

```


Let's upload the dataset and see what we got here:
```{r}
data <- read.csv("atp_matches_2015.csv")

#rename variable column names

data$p1_seed <- data$winner_seed
data$p1_entry <- data$winner_entry
data$p1_name <- data$winner_name
data$p1_hand <- data$winner_hand
data$p1_ht <- data$winner_ht
data$p1_age <- data$winner_age
data$p1_rank <- data$winner_rank
data$p1_rank_points <- data$winner_rank_points

data$p2_seed <- data$loser_seed
data$p2_entry <- data$loser_entry
data$p2_name <- data$loser_name
data$p2_hand <- data$loser_hand
data$p2_ht <- data$loser_ht
data$p2_age <- data$loser_age
data$p2_rank <- data$loser_rank
data$p2_rank_points <- data$loser_rank_points

data$p1_ace <- data$w_ace
data$p1_df <- data$w_df
data$p1_svpt <- data$w_svpt
data$p1_1stIn <- data$w_1stIn
data$p1_1stWon <- data$w_1stWon
data$p1_2ndWon <- data$w_2ndWon
data$p1_SvGms <- data$w_SvGms
data$p1_bpSaved <- data$w_bpSaved
data$p1_bpFaced <- data$w_bpFaced

data$p2_ace <- data$l_ace
data$p2_df <- data$l_df
data$p2_svpt <- data$l_svpt
data$p2_1stIn <- data$l_1stIn
data$p2_1stWon <- data$l_1stWon
data$p2_2ndWon <- data$l_2ndWon
data$p2_SvGms <- data$l_SvGms
data$p2_bpSaved <- data$l_bpSaved
data$p2_bpFaced <- data$l_bpFaced

#get rid of useless info

data$tourney_id <- NULL
data$tourney_name <- NULL
data$surface <- NULL
data$draw_size <- NULL
data$tourney_level <- NULL
data$tourney_date <- NULL
data$match_num <- NULL
data$winner_id <- NULL
data$loser_id <- NULL
data$winner_ioc <- NULL
data$loser_ioc <- NULL
data$round <- NULL
data$minutes <- NULL
data$best_of <- NULL
data$score <- NULL

data$winner_seed <- NULL
data$winner_entry <- NULL
data$winner_name <- NULL
data$winner_hand <- NULL
data$winner_ht <- NULL
data$winner_age <- NULL
data$winner_rank <- NULL
data$winner_rank_points <- NULL

data$loser_seed <- NULL
data$loser_entry <- NULL
data$loser_name <- NULL
data$loser_hand <- NULL
data$loser_ht <- NULL
data$loser_age <- NULL
data$loser_rank <- NULL
data$loser_rank_points <- NULL

data$w_ace <- NULL
data$w_df <- NULL
data$w_svpt <- NULL
data$w_1stIn <- NULL
data$w_1stWon <- NULL
data$w_2ndWon <- NULL
data$w_SvGms <- NULL
data$w_bpSaved <- NULL
data$w_bpFaced <- NULL

data$l_ace <- NULL
data$l_df <- NULL
data$l_svpt <- NULL
data$l_1stIn <- NULL
data$l_1stWon <- NULL
data$l_2ndWon <- NULL
data$l_SvGms <- NULL
data$l_bpSaved <- NULL
data$l_bpFaced <- NULL

#add outcome variable

data$Outcome <- 1

#change types of variables

data$Outcome <- as.integer(data$Outcome)

#get rid of NA's, questionable step here as it takes the data set to 305 records from about 3000

data <- na.omit(data)

```

Get rid of non significant variables
```{r}
data$p1_name <- NULL
data$p2_name <- NULL

```

Swap half of dataframe 
```{r}
rows <- nrow(data)
secondhalfdata <- rows/2

for (x in secondhalfdata:rows){
  
  p1_seed_temp <- data$p1_seed[x]
  data$p1_seed[x] <- data$p2_seed[x]
  data$p2_seed[x] <- p1_seed_temp
  
  p1_entry_temp <- data$p1_entry[x]
  data$p1_entry[x] <- data$p2_entry[x]
  data$p2_entry[x] <- p1_entry_temp
  
  p1_hand_temp <- data$p1_hand[x]
  data$p1_hand[x] <- data$p2_hand[x]
  data$p2_hand[x] <- p1_hand_temp
  
  p1_ht_temp <- data$p1_ht[x]
  data$p1_ht[x] <- data$p2_ht[x]
  data$p2_ht[x] <- p1_ht_temp
  
  p1_age_temp <- data$p1_age[x]
  data$p1_age[x] <- data$p2_age[x]
  data$p2_age[x] <- p1_age_temp  
  
  p1_rank_temp <- data$p1_rank[x]
  data$p1_rank[x] <- data$p2_rank[x]
  data$p2_rank[x] <- p1_rank_temp  
  
  p1_rank_points_temp <- data$p1_rank_points[x]
  data$p1_rank_points[x] <- data$p2_rank_points[x]
  data$p2_rank_points[x] <- p1_rank_points_temp  
  
  p1_ace_temp <- data$p1_ace[x]
  data$p1_ace[x] <- data$p2_ace[x]
  data$p2_ace[x] <- p1_ace_temp  
  
  p1_df_temp <- data$p1_df[x]
  data$p1_df[x] <- data$p2_df[x]
  data$p2_df[x] <- p1_df_temp  
  
  p1_svpt_temp <- data$p1_svpt[x]
  data$p1_svpt[x] <- data$p2_svpt[x]
  data$p2_svpt[x] <- p1_svpt_temp
  
  p1_1stIn_temp <- data$p1_1stIn[x]
  data$p1_1stIn[x] <- data$p2_1stIn[x]
  data$p2_1stIn[x] <- p1_1stIn_temp  
  
  p1_1stWon_temp <- data$p1_1stWon[x]
  data$p1_1stWon[x] <- data$p2_1stWon[x]
  data$p2_1stWon[x] <- p1_1stWon_temp
  
  p1_2ndWon_temp <- data$p1_2ndWon[x]
  data$p1_2ndWon[x] <- data$p2_2ndWon[x]
  data$p2_2ndWon[x] <- p1_2ndWon_temp 
  
  p1_SvGms_temp <- data$p1_SvGms[x]
  data$p1_SvGms[x] <- data$p2_SvGms[x]
  data$p2_SvGms[x] <- p1_SvGms_temp
  
  p1_bpSaved_temp <- data$p1_bpSaved[x]
  data$p1_bpSaved[x] <- data$p2_bpSaved[x]
  data$p2_bpSaved[x] <- p1_bpSaved_temp
  
  p1_bpFaced_temp <- data$p1_bpFaced[x]
  data$p1_bpFaced[x] <- data$p2_bpFaced[x]
  data$p2_bpFaced[x] <- p1_bpFaced_temp
  
  data$Outcome[x] <- 0
  
}
```

Deal with seeds, ht, and entry, this effectively limits the dataset size

```{r}
#for (x in nrow(data)){
#  if (data$p1_seed[x] == "NA"){
#    data$p1_seed[x] <- 0
#  }
#  if (data$p2_seed[x] == "NA"){
#    data$p2_seed[x] <- 0
#  }
#  if (data$p1_ht[x] == "NA"){
#    data$p1_ht[x] <- 0
#  }
#  if (data$p1_ht[x] == "NA"){
#    data$p1_ht[x] <- 0
#  }
#  if (data$p1_entry[x] == ""){
#    data$p1_entry[x] <- U
#  }
#}

```

Get the deltas of the data

```{r}
#Get Deltas of match data

for (x in 1:nrow(data)){
  
  data$delta_seed[x] <- data$p1_seed[x] - data$p2_seed[x]
  data$delta_ht[x] <- data$p1_ht[x] - data$p2_ht[x]
  data$delta_age[x] <- data$p1_age[x] - data$p2_age[x]
  data$delta_rank[x] <- data$p1_rank[x] - data$p2_rank[x]
  data$delta_rank_points[x] <- data$p1_rank_points[x] - data$p2_rank_points[x]
  data$delta_ace[x] <- data$p1_ace[x] - data$p2_ace[x]
  data$delta_df[x] <- data$p1_df[x] - data$p2_df[x]
  data$delta_1stIn[x] <- data$p1_1stIn[x] - data$p2_1stIn[x]
  data$delta_1stWon[x] <- data$p1_1stWon[x] - data$p2_1stWon[x]
  data$delta_2ndWon[x] <- data$p1_2ndWon[x] - data$p2_2ndWon[x]
  data$delta_SvGms[x] <- data$p1_SvGms[x] - data$p2_SvGms[x]
  data$delta_bpSaved[x] <- data$p1_bpSaved[x] - data$p2_bpSaved[x]
  data$delta_bpFaced[x] <- data$p1_bpFaced[x] - data$p2_bpFaced[x]
  
}

```

Put together a true delta data set

```{r}

delta_data <- data[c(2,3,9,10,33:46)]

```

Run a logistic regression to see whats significant

```{r}
#preliminary statistical analysis using logistic regression

#commented out as this was just to test if the df_train data was different in some way, its not
#df_train$Outcome <- as.numeric(df_train$Outcome)

logistic_model <- glm(Outcome ~  ., data = delta_rand)
summary(logistic_model)
?glm
```

In this code block we are preparing our data for use in machine learning algorithms.

### Data Management

  - Randomize your dataset
  - Break into test and train datasets
  - Check and ensure that test and train datasets are similar
  
```{r}
#Data Prep for models

#randomize
delta_rand <- delta_data[sample(nrow(delta_data)),]

#make Outcome a factor
delta_rand$Outcome <- as.factor(delta_rand$Outcome)
                                                                                                          
# #Sample Indexes
indexes = sample(1:nrow(delta_rand), size=0.25*nrow(delta_rand))
  
# # Split data
df_test <- delta_rand[indexes,]
dim(df_test)
df_train <- delta_rand[-indexes,]
dim(df_train) 

summary(df_train)
summary(df_test)
#Thanks to randomization, it appears that the proportion of -1 to 1 responses where there are two level factors is about equal across the columns.
```
  
### Predictive Modeling

Build (and attempt to improve) **any three** of the predictive models listed below:

  - KNN: My KNN model returned an accuracy of 96.24% and a kappa of .9241.
  
```{r}
#KNN
knn_train <- model.matrix(~.-1, data = df_train)
knn_test <- model.matrix(~.-1,data = df_test)

knn_train_labels <- knn_train[, 5]
knn_test_labels <- knn_test[, 5]

knn_test_pred <- knn(train = knn_train, test = knn_test,
                      cl = knn_train_labels, k=1)

CrossTable(x = knn_test_labels, y = knn_test_pred, 
           prop.chisq=FALSE,prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))

confusionMatrix(data = knn_test_pred, reference = knn_test_labels, positive = "1")

```
  
  - SVM: My SVM's (I ran two, a vanilladot and rbfdot) returned an accuracy of 93.67% and 95.62% respectively. They also returned kappas of .8713 (vanilla) and .9111 (rbf). As a result we can see that the rbf is the best of the two and should be used in the voting system vs the vanillas.
  
```{r}
#SVM

#Vanilla SVM
SVM_classifier <- ksvm(Outcome ~ ., data = df_train,
                          kernel = "vanilladot")

# look at basic information about the model
SVM_classifier

## Evaluating model performance 
# predictions on testing dataset
SVM_predictions <- predict(SVM_classifier, df_test)

head(SVM_predictions)

table(SVM_predictions, df_test$Outcome)

# look only at agreement vs. non-agreement
# construct a vector of TRUE/FALSE indicating correct/incorrect predictions
agreement <- SVM_predictions == df_test$Outcome
table(agreement)
prop.table(table(agreement))

confusionMatrix(data = SVM_predictions, reference = df_test$Outcome, positive = "1")

## Improving model performance
SVM_classifier_rbf <- ksvm(Outcome ~ ., data = df_train, kernel = "rbfdot")
SVM_predictions_rbf <- predict(SVM_classifier_rbf, df_test)

agreement_rbf <- SVM_predictions_rbf == df_test$Outcome
table(agreement_rbf)
prop.table(table(agreement_rbf))

confusionMatrix(data = SVM_predictions_rbf, reference = df_test$Outcome, positive = "1")

```
  
  - ANN (not too many hidden layers): My neural net with 10 folds of cross validation returned an accuracy of 95.73% and a kappa of .9135.
  
```{r}
model_nnet <- train(Outcome ~ ., df_train, method = "nnet", trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE))

prediction_nnet <- predict(model_nnet, df_test, na.rm = TRUE)

CrossTable(x = df_test$Outcome, y = prediction_nnet, 
           prop.chisq=FALSE,prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))

confusionMatrix(data = prediction_nnet, reference = df_test$Outcome, positive = "1")
```  

  - Decision Trees: My basic Decision Tree returned an accuracy of 95.91% with a kappa of .9172, solid. My boosted model had an accuray of 97.03% and a kappa of .9399, better than my basic one. My cost matrix decision tree returned an accuracy of 95.66% and a kappa of .912, worse than the two others created. My last model with 10 folds of cross validation returned an accuracy of 84.92%, definitely the worst and won't be using.
  
```{r}
#Decision Trees

#I'm going to build four decision tree models, a basic, boosted, one with a cost matrix, and a cross validated one. I will then pick the model that is the most accurate as the one I will plug into my voting system below.

#Basic Decision Tree
DT_model <- C5.0(df_train[-5], df_train$Outcome)

# display simple facts about the tree
DT_model

# display detailed information about the tree
summary(DT_model)

# create a factor vector of predictions on test data
DT_pred <- predict(DT_model, df_test)

# cross tabulation of predicted versus actual classes
CrossTable(df_test$Outcome, DT_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))

confusionMatrix(data = DT_pred, reference = df_test$Outcome, positive = "1")

#Decision tree model with 10 trials adaptive boosting:
Result_boost10 <- C5.0(df_train[-5], df_train$Outcome,
                       trials = 10)
Result_boost10
summary(Result_boost10)

Result_boost_pred10 <- predict(Result_boost10, df_test)
CrossTable(df_test$Outcome, Result_boost_pred10,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))

confusionMatrix(data = Result_boost_pred10, reference = df_test$Outcome, positive = "1")


#Now I will create a cost matrix that makes certain outcomes worse than over other outcomes. In this case my cost matrix weights errors that return a false negative as worse than a false positive. In this case I think this is the way to go becuase if this code was to bused for a product you would rather have the product be a bit overprotective, especially if it was guarding financial or critical information.

#cost matrix
Result_cost <- matrix(c(0, 2, 3, 0), nrow = 2)
Result_cost

# apply the cost matrix to the tree
Result_cost <- C5.0(df_train[-5], df_train$Result,
                          costs = Result_cost)
Result_costmatrix_pred <- predict(Result_cost, df_test)

CrossTable(df_test$Outcomet, Result_costmatrix_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))

confusionMatrix(data = Result_costmatrix_pred, reference = df_test$Outcome, positive = "1")

#I'll now create a 10 fold cross validation Decision Tree model.

## Automating 10-fold CV for a C5.0 Decision Tree using lapply() ----
set.seed(123)
folds <- createFolds(df_rand$Outcome, k = 10)

cv_results <- lapply(folds, function(x) {
  df_rand_train <- df_rand[x, ]
  df_rand_test <- df_rand[-x, ]
  df_rand_model <- C5.0(Result ~ ., data = df_rand_train)
  df_rand_pred <- predict(df_rand_model, df_rand_test)
  df_rand_actual <- df_rand_test$Outcome
  kappa <- kappa2(data.frame(df_rand_actual,df_rand_pred))$value
  return(kappa)
})

str(cv_results)
mean(unlist(cv_results))

```
  
  - Random Forest: My random forest had an accuracy of 96.74% and a Kappa of .934. Based on these values I would say that this model is operating decently well.
  
```{r}
## Random Forest 

set.seed(300)
rf <- randomForest(Outcome ~ ., data = df_train)

rf_predictions <- predict(rf, df_test)

CrossTable(x = df_test$Outcome, y = rf_predictions, 
           prop.chisq=FALSE,prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))

confusionMatrix(data = rf_predictions, reference = df_test$Outcome)
```

For the models that you build, calculate the Confusion Matrix and the Kappa statistic. Write a summary of how good or bad your models are.

### Combining Models

You are asked to combine the three models that you have built in the section above using a simple voting scheme (majority rules). Calculate the Confusion Matrix and Kappa statistic for the combined model.

The voting system that takes the results of all five models returns an accuracy of 95.9% and a kappa of .9173.

```{r}
voting_prediction <- as.data.frame(cbind(prediction_nnet, DT_pred, rf_predictions, SVM_predictions_rbf, knn_test_pred, df_test$Result) - 1)

names(voting_prediction)[6] <- "Actual_Outcome"

voting_prediction$final_prediction <- ifelse(voting_prediction[1] + voting_prediction[2] + voting_prediction[3] + voting_prediction[4] + voting_prediction[5] >= 3, 1,0)

voting_prediction$correct <- ifelse(voting_prediction[7] == voting_prediction[6], 1, 0)

sum(voting_prediction$correct) / nrow(voting_prediction)

confusionMatrix(data = voting_prediction$final_prediction, reference = voting_prediction$Actual_Outcome)

```


### What Does It All Mean?

Does your work here has any business relevance? Who can be an audience for your model? How can your model be used in actual practice? How can you monetize your analysis in this exam?

Yes this definitely has business relevance!!! I think a lot of email services employ something exactly like this to flag emails that be potentially dangerous. It could potentially increase the security of companies and governments by trying to decrease the number of times hackers or people with malicious entent are able to gain access to a company's intranet via a phishing scam. In actualy practice it could be used to automate which emails are flagged or not. 

There are many many ways that this could be monetized including: 1) Build your own google chrome email extension that analyzes the content of emails and then generates its own warning for users, 2) Improve this algorithm and sell it to Google, Yahoo! etc 3) Build an actual email application that analyzes all of the content locally rather than on the internet via a chrome extension 4) WORK FOR THE DARK SIDE, collaborate with hackers to develop even better phishing scams so we can seize critical information, login credentials, bank account info etc and either ransom it back to their owners or use it maliciously 5) Work for the national security agency to develop evil phishing schemes for good (or so they say). 6) Help your younger siblings target the other local lemonade stands and disrupt their Amazon cup supply network by extracting and changing their login passwords, thus establishing your younger siblings lemonade stand as the most reliable one in the neighberhood, feel free to take a cut of the profits.

> That's it. Upload your work to Canvas.
 
